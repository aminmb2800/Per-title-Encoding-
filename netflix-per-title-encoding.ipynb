{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Packages","metadata":{}},{"cell_type":"markdown","source":"# Video Quality Metric prediction\n\n\n## Data \n\nData is one csv file which contains four sections Source,Video Characteristic,Encoding Setting and Target Value  \n\n- **Video_data_set.csv** - A collected data from videos \n\n- **Source Viedo**\n \n  * `s_video_id` - Identifer for the original video (numerical)\n  * `s_width` - Resolution of the raw video (numerical)\n  * `s_height` - Resolution of the raw video (numerical)\n  * `s_storage_size` - the total size of the source video without audio tracks (numerical)\n  * `s_duration` - Length of the source video (numerical)\n  * `s_scan_type` - \"progressive\" or \"interlaced\"(categorical)\n  \n**----------------------------------------------------------------------------------**  \n  \n- **Video Characteristic**\n\n  * `c_content_category` - A label indicating the category of the video with the highest probability(categorical)\n  * `c_scene_change_ffmp_ratio30` - Indicates how many scene changes appear per minute on average in the video for a given probability 30%(numerical)\n  * `c_scene_change_ffmp_ratio60` - Indicates how many scene changes appear per minute on average in the video for a given probability 60%(numerical)\n  * `c_scene_change_ffmp_ratio90` - Indicates how many scene changes appear per minute on average in the video for a given probability 90%(numerical)\n  * `c_scene_change_py_thresh30` - Indicates how many scene changes appear throughout entire clip with threshold 30(numerical)\n  * `c_scene_change_py_thresh50` - Indicates how many scene changes appear throughout entire clip with threshold 50(numerical)\n  * `c_si` - The spatial perceptual information (SI) based on the Sobel filter averaged over the whole video(numerical)\n  * `c_ti` - The temporal perceptual information (TI) based upon the motion difference feature averaged over the whole video(numerical)\n  * `c_colorhistogram_mean_dark` - color values between [0-63] are grouped in this block (dark)population mean of RGB color values normalised - divide by pixel count & divide by channel count mean of mean over all frames of a video(numerical)\n  * `c_colorhistogram_mean_medium_dark` - color values between [64-127] (numerical)\n  * `c_colorhistogram_mean_medium_bright` - color values between [128-195] (numerical)\n  * `c_colorhistogram_mean_bright` - color values between [196-255] (numerical)\n  * `c_colorhistogram_std_dev_dark` - standard deviation of c_colorhistogram_mean_medium_dark within each frame mean of all frames of a video(numerical)\n  * `c_colorhistogram_std_dev_medium_dark` - standard deviation of c_colorhistogram_mean_medium_bright within each frame mean of all frames of a video(numerical)\n  * `c_colorhistogram_std_dev_medium_bright` - standard deviation of c_colorhistogram_mean_medium_bright within each frame mean of all frames of a video(numerical)\n  * `c_colorhistogram_std_dev_bright` - standard deviation of c_colorhistogram_mean_bright within each frame mean of all frames of a video(numerical)\n  * `c_colorhistogram_temporal_mean_std_dev_dark` - temporal standard deviation of mean of c_colorhistogram_mean_dark(numerical)\n  * `c_colorhistogram_temporal_mean_std_dev_medium_dark` - temporal standard deviation of mean of c_colorhistogram_mean_medium_dark(numerical)\n  * `c_colorhistogram_temporal_mean_std_dev_medium_bright` - temporal standard deviation of mean of c_colorhistogram_mean_medium_bright(numerical)\n  * `c_colorhistogram_temporal_mean_std_dev_bright` - temporal standard deviation of mean of c_colorhistogram_mean_bright(numerical)\n- **Encoding Setting**\n  * `e_crf` - Constant Rate Factor for this encoding(numerical)\n  * `e_width` - Target Resolution of the encoded video(numerical)\n  * `e_height` - Target Resolution of the encoded video(numerical)\n  * `e_aspect_ratio` - Aspect ratio of the video(numerical)\n  * `e_pixel_aspect_ratio` - Aspect ratio of the pixels. Usually 1:1 = 1(numerical)\n  * `e_codec` - Video Codec e.g H.264, H.265, VP9, AV1.(categorical)\n  * `e_codec_profile` - Video Codec Profile e.g baseline, main, high. Depending on the profile certain encoder features are disabled/enabled(categorical)\n  * `e_codec_level` - Video Codec Level: Specified set of constraints that indicate a degree of required decoder performance for a profile.(ordered categorical)\n  * `e_framerate` - Frames per second(numerical)\n  * `e_gop_size` - number of frames between two I-frames(numerical)\n  * `e_b_frame_int` - number of b frames per interval(numerical)\n  * `e_ref_frame_count` - Reference frames are frames of a compressed video that are used to define future frames.(numerical)\n  * `e_scan_type` - \"progressive\" or \"interlaced\"(categorical)\n  * `e_bit_depth` - Amount of information stored in each pixel of data. also known as 'color depth'(numerical)\n  * `e_pixel_fmt` - color models like YUV, RGB, YPbPr, etc.(categorical)\n\n- **Target Value**\n  * `t_average_bitrate` - Average Bitrate as encoding setting(numerical)\n  * `t_average_vmaf` - quality metric((numerical)\n  * `t_average_vmaf_mobile` - No Value \n  * `t_average_vmaf_4k` - No Value \n  * `t_average_psnr` - No Value ","metadata":{}},{"cell_type":"markdown","source":"# Import Packages","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\n\nfrom math import sqrt\nfrom pprint import pprint\nfrom IPython.display import display\nfrom scipy.spatial import ConvexHull\nfrom matplotlib.pyplot import figure\n\nfrom sklearn.metrics import r2_score\nfrom sklearn import preprocessing, svm\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_absolute_error as MAE\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.model_selection import train_test_split,RandomizedSearchCV\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\nwarnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")","metadata":{"execution":{"iopub.status.busy":"2022-04-09T09:44:09.590150Z","iopub.execute_input":"2022-04-09T09:44:09.590740Z","iopub.status.idle":"2022-04-09T09:44:09.600563Z","shell.execute_reply.started":"2022-04-09T09:44:09.590705Z","shell.execute_reply":"2022-04-09T09:44:09.599730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read the Dataset","metadata":{}},{"cell_type":"code","source":"data_set = pd.read_csv(\"../input/per-title-encoding/Video_data_set.csv\",sep = ',')\nprint(\"**Video Dataset:**\")\ndisplay(data_set.head())\nprint(\"**Video Dataset shape:**\", data_set.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T09:44:13.486764Z","iopub.execute_input":"2022-04-09T09:44:13.487348Z","iopub.status.idle":"2022-04-09T09:44:13.607133Z","shell.execute_reply.started":"2022-04-09T09:44:13.487293Z","shell.execute_reply":"2022-04-09T09:44:13.606288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing ","metadata":{}},{"cell_type":"markdown","source":"## Droping unneccesary data","metadata":{}},{"cell_type":"markdown","source":"After searching about the data in the dataset, \nI found that some data does not affect on the our target which is **VMAF**.\n**Reason:** \n- The VMAF value is the numerical target and also the values that belongs to these data are showing most format and structure of data. ","metadata":{}},{"cell_type":"code","source":"data_set = data_set.drop([\"e_codec\",\n                          \"e_codec_profile\",\n                          \"e_scan_type\",\n                          \"e_pixel_aspect_ratio\",\n                          \"e_pixel_fmt\",\n                          \"e_aspect_ratio\",\n                          \"e_b_frame_int\",\n                          \"e_ref_frame_count\",\n                          \"t_average_vmaf_mobile\",\n                          \"e_bit_depth\",\n                          \"t_average_vmaf_4k\",\n                          \"t_average_psnr\",\n                          \"s_scan_type\"], axis=1) # remove desire columns \ndata_set.head()\nprint(\"**Video Dataset shape after removing columns:**\", data_set.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T09:45:28.011780Z","iopub.execute_input":"2022-04-09T09:45:28.012083Z","iopub.status.idle":"2022-04-09T09:45:28.025408Z","shell.execute_reply.started":"2022-04-09T09:45:28.012041Z","shell.execute_reply":"2022-04-09T09:45:28.024543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Cleaning ","metadata":{}},{"cell_type":"markdown","source":"The **data_set** contains Nan value and also some duplicate value in below columns names: \n- `c_content_category`          \n- `c_colorhistogram_mean_dark`                             \n- `c_colorhistogram_mean_medium_dark`                       \n- `c_colorhistogram_mean_medium_bright`                     \n- `c_colorhistogram_mean_bright`                            \n- `c_colorhistogram_std_dev_dark`                           \n- `c_colorhistogram_std_dev_medium_dark`                    \n- `c_colorhistogram_std_dev_medium_bright`                  \n- `c_colorhistogram_std_dev_bright`                         \n- `c_colorhistogram_temporal_mean_std_dev_dark`             \n- `c_colorhistogram_temporal_mean_std_dev_medium_dark`      \n- `c_colorhistogram_temporal_mean_std_dev_medium_bright`    \n- `c_colorhistogram_temporal_mean_std_dev_bright`\n- `c_ti`\n- `c_si`\n- `e_codec_level`                             \n- `e_framerate`                                             \n- `e_gop_size`\n- `t_average_vmaf`\n\nIn next cell, I remove all duplicates and Nan values and sort them base on Video ID.  ","metadata":{}},{"cell_type":"code","source":"data_set_c = data_set.copy()\ndata_set_c.drop_duplicates(inplace=True)    # Removing the duplicate rows\ndata_set_c.dropna(inplace=True)             # Removing the Nan value \ndata_set_c.sort_values(by=['s_video_id'], inplace=True)  # sort dataset based on video ID  \ndata_set_c.reset_index(inplace = True, drop = True)  \nprint(\"**Dataset shape after data cleaning:**\", data_set_c.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T09:46:48.874290Z","iopub.execute_input":"2022-04-09T09:46:48.874609Z","iopub.status.idle":"2022-04-09T09:46:48.909521Z","shell.execute_reply.started":"2022-04-09T09:46:48.874579Z","shell.execute_reply":"2022-04-09T09:46:48.908614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Prepration","metadata":{}},{"cell_type":"markdown","source":"- Since the dataset contains the `Height` and `Width` so we can have the resolution of the videos as a new column named as `Res_width_height` \n\n- Calculate the Scene change in second based on the duration also can help to have the average scene change and named as `scene_change_avg` in dataset.   ","metadata":{}},{"cell_type":"code","source":"# Calculate resolution \ndata_set_c['Res_width_height'] = data_set_c['e_width']*data_set_c['e_height']","metadata":{"execution":{"iopub.status.busy":"2022-04-09T09:48:13.792665Z","iopub.execute_input":"2022-04-09T09:48:13.792977Z","iopub.status.idle":"2022-04-09T09:48:13.800607Z","shell.execute_reply.started":"2022-04-09T09:48:13.792944Z","shell.execute_reply":"2022-04-09T09:48:13.799655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# division of each scene by duration in seconds\ndata_set_c[\"scene_change_avg\"]= data_set_c['c_scene_change_ffmpeg_ratio60']*60/data_set_c['s_duration']","metadata":{"execution":{"iopub.status.busy":"2022-04-09T09:48:23.472118Z","iopub.execute_input":"2022-04-09T09:48:23.472411Z","iopub.status.idle":"2022-04-09T09:48:23.478546Z","shell.execute_reply.started":"2022-04-09T09:48:23.472378Z","shell.execute_reply":"2022-04-09T09:48:23.477811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Detecting outliers and Apply method ITU-T P.1401 for removing ","metadata":{}},{"cell_type":"markdown","source":"- In order to remove the extreme outliers,the technique ITU-T Recommendation P.1401 was\nfollowed, which is basically for subjective rating, but I\nfound it suitable for the data as well. In this method,\nunlike the general Box and whiskers graphs in which\ndata out of the range 1.5 times the Interquartile Range\n(IQR) considering as outliers, 3 times the IQR used to find\nand remove the extreme outliers.\n- Applying this method on features that more important for our scenario and removing the outliers \n\n**Applied features:** \n\n- `t_average_bitrate`\n- `scene_change_avg`\n- `s_duration`\n- `s_storage_size`","metadata":{}},{"cell_type":"markdown","source":"**In below, I applied boxplot inorder to check the outliers \n(outliers are points that are out of range data distribution)**","metadata":{}},{"cell_type":"code","source":"sns.boxplot(x= data_set_c['t_average_bitrate'])","metadata":{"execution":{"iopub.status.busy":"2022-04-09T09:53:24.365007Z","iopub.execute_input":"2022-04-09T09:53:24.365285Z","iopub.status.idle":"2022-04-09T09:53:24.587830Z","shell.execute_reply.started":"2022-04-09T09:53:24.365256Z","shell.execute_reply":"2022-04-09T09:53:24.587112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Remove outliers of `average_bitrate` by ITU-T method**","metadata":{}},{"cell_type":"code","source":"shape=data_set_c['t_average_bitrate'].shape[0] # get the number of row \nI_range=data_set_c['t_average_bitrate'].describe()[6]-data_set_c['t_average_bitrate'].describe()[4]\nI_range2=I_range*3  \nmax_threshold=data_set_c['t_average_bitrate'].describe()[6]+I_range2\nfor i in range(shape):\n    if(data_set_c.iloc[i, 31:32].values[0] >= max_threshold):\n        data_set_c.iloc[i, 31:32]= np.nan\n\n\ndata_set_c.dropna(inplace=True)\ndata_set_c.reset_index(inplace= True, drop=True)\ndisplay(data_set_c.head())\nprint(\"**Dataset shape after removing outlier of Bitrate:**\", data_set_c.shape)\nprint(\"-----------------------------------------------------\")\nprint(\"Checking outlier of 'Bitrate'after removing outliers \")\nsns.boxplot(x= data_set_c['t_average_bitrate'])\n","metadata":{"execution":{"iopub.status.busy":"2022-04-09T09:53:55.273725Z","iopub.execute_input":"2022-04-09T09:53:55.274183Z","iopub.status.idle":"2022-04-09T09:53:57.658186Z","shell.execute_reply.started":"2022-04-09T09:53:55.274133Z","shell.execute_reply":"2022-04-09T09:53:57.657364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Remove outliers of `scene_change_average` by ITU-T method**","metadata":{}},{"cell_type":"code","source":"sns.boxplot(x= data_set_c['scene_change_avg'])","metadata":{"execution":{"iopub.status.busy":"2022-04-09T09:54:29.185322Z","iopub.execute_input":"2022-04-09T09:54:29.185619Z","iopub.status.idle":"2022-04-09T09:54:29.369493Z","shell.execute_reply.started":"2022-04-09T09:54:29.185590Z","shell.execute_reply":"2022-04-09T09:54:29.368804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shape=data_set_c['scene_change_avg'].shape[0]\nI_range=data_set_c['scene_change_avg'].describe()[6]-data_set_c['scene_change_avg'].describe()[4]\nI_range2=I_range*3\nmax_threshold=data_set_c['scene_change_avg'].describe()[6]+I_range2\nfor i in range(shape):\n    if(data_set_c.iloc[i, 34:35].values[0] >= max_threshold):\n        data_set_c.iloc[i, 34:35]=np.nan\n\ndata_set_c.dropna(inplace=True)        \ndata_set_c.reset_index(inplace= True, drop=True)\ndisplay(data_set_c.head())\nprint(\"**Dataset shape after removing outlier of scene_change:**\", data_set_c.shape)\nprint(\"----------------------------------------------------------\")\nprint(\"Checking outlier of 'scene_change'after removing outliers \")\nsns.boxplot(x= data_set_c['scene_change_avg'])","metadata":{"execution":{"iopub.status.busy":"2022-04-09T09:54:41.294487Z","iopub.execute_input":"2022-04-09T09:54:41.295264Z","iopub.status.idle":"2022-04-09T09:54:43.409205Z","shell.execute_reply.started":"2022-04-09T09:54:41.295207Z","shell.execute_reply":"2022-04-09T09:54:43.408449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Remove outliers of `s_duration` by ITU-T method**","metadata":{"execution":{"iopub.status.busy":"2022-04-09T09:54:59.470460Z","iopub.execute_input":"2022-04-09T09:54:59.470748Z","iopub.status.idle":"2022-04-09T09:54:59.476481Z","shell.execute_reply.started":"2022-04-09T09:54:59.470720Z","shell.execute_reply":"2022-04-09T09:54:59.475475Z"}}},{"cell_type":"code","source":"sns.boxplot(x= data_set_c['s_duration'])","metadata":{"execution":{"iopub.status.busy":"2022-04-09T09:55:11.965702Z","iopub.execute_input":"2022-04-09T09:55:11.966509Z","iopub.status.idle":"2022-04-09T09:55:12.139289Z","shell.execute_reply.started":"2022-04-09T09:55:11.966462Z","shell.execute_reply":"2022-04-09T09:55:12.138473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shape=data_set_c['s_duration'].shape[0]\nI_range=data_set_c['s_duration'].describe()[6]-data_set_c['s_duration'].describe()[4]\nI_range2=I_range*3\nmax_threshold=data_set_c['s_duration'].describe()[6]+I_range2\nfor i in range(shape):\n    if(data_set_c.iloc[i, 4:5].values[0] >= max_threshold):\n        data_set_c.iloc[i, 4:5]=np.nan\n\ndata_set_c.dropna(inplace=True)        \ndata_set_c.reset_index(inplace= True, drop=True)\ndisplay(data_set_c.head())\nprint(\"**Dataset shape after removing outlier of video duration:**\", data_set_c.shape)\nprint(\"----------------------------------------------------------\")\nprint(\"Checking outlier of 'duration'after removing outliers \")\nsns.boxplot(x= data_set_c['s_duration'])","metadata":{"execution":{"iopub.status.busy":"2022-04-09T09:55:24.559693Z","iopub.execute_input":"2022-04-09T09:55:24.560164Z","iopub.status.idle":"2022-04-09T09:55:26.768679Z","shell.execute_reply.started":"2022-04-09T09:55:24.560111Z","shell.execute_reply":"2022-04-09T09:55:26.768007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Remove outliers of `s_storage_size` by ITU-T method**","metadata":{}},{"cell_type":"code","source":"sns.boxplot(x= data_set_c['s_storage_size'])","metadata":{"execution":{"iopub.status.busy":"2022-04-09T09:56:04.194433Z","iopub.execute_input":"2022-04-09T09:56:04.194726Z","iopub.status.idle":"2022-04-09T09:56:04.371172Z","shell.execute_reply.started":"2022-04-09T09:56:04.194690Z","shell.execute_reply":"2022-04-09T09:56:04.370372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shape=data_set_c['s_storage_size'].shape[0]\nI_range=data_set_c['s_storage_size'].describe()[6]-data_set_c['s_duration'].describe()[4]\nI_range2=I_range*3\nmax_threshold=data_set_c['s_storage_size'].describe()[6]+I_range2\nfor i in range(shape):\n    if(data_set_c.iloc[i, 3:4].values[0] >= max_threshold):\n        data_set_c.iloc[i, 3:4]=np.nan\n\ndata_set_c.dropna(inplace=True)        \ndata_set_c.reset_index(inplace= True, drop=True)\ndisplay(data_set_c.head())\nprint(\"**Dataset shape after removing outlier of storage_size:**\", data_set_c.shape)\nprint(\"----------------------------------------------------------\")\nprint(\"Checking outlier of 'storage size'after removing outliers \")\nsns.boxplot(x= data_set_c['s_storage_size'])","metadata":{"execution":{"iopub.status.busy":"2022-04-09T09:56:13.197619Z","iopub.execute_input":"2022-04-09T09:56:13.198155Z","iopub.status.idle":"2022-04-09T09:56:15.049347Z","shell.execute_reply.started":"2022-04-09T09:56:13.198116Z","shell.execute_reply":"2022-04-09T09:56:15.048523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Description ","metadata":{}},{"cell_type":"code","source":"display(data_set_c.describe().transpose()) ","metadata":{"execution":{"iopub.status.busy":"2022-04-09T09:56:46.749001Z","iopub.execute_input":"2022-04-09T09:56:46.749658Z","iopub.status.idle":"2022-04-09T09:56:46.977123Z","shell.execute_reply.started":"2022-04-09T09:56:46.749621Z","shell.execute_reply":"2022-04-09T09:56:46.976460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to understand more relation of the fetures with **VMAF**, \nI applied the correlation heatmap between the them so I can choose the most correlated one. ","metadata":{}},{"cell_type":"code","source":"corrMat = data_set_c.corr(method='pearson') #correlation calculation\nfig, ax = plt.subplots(figsize=(20, 20))\nsns.heatmap(corrMat, annot=True, fmt='.2f', ax=ax) # base on the heatmap\nplt.title(\"Correlation Matrix after preprocessing \")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T09:57:15.838218Z","iopub.execute_input":"2022-04-09T09:57:15.839079Z","iopub.status.idle":"2022-04-09T09:57:20.951063Z","shell.execute_reply.started":"2022-04-09T09:57:15.839043Z","shell.execute_reply":"2022-04-09T09:57:20.950337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparation of Train / Test ","metadata":{}},{"cell_type":"markdown","source":"After checking the correlation between the data and also based on the papers that shared\nfrom Netflix, I applied the high correlated features into the Machine learning which are: \n- `scene_change_avg`,`s_duration`,`e_framerate`\n- `s_height`,`s_storage_size`,`Res_width_height`,`t_average_bitrate`\n- `e_crf`,`s_width`,`t_average_vmaf`","metadata":{}},{"cell_type":"markdown","source":"Getting the features value from the preprocessed `data_set_c` into `data_set_final`  ","metadata":{}},{"cell_type":"code","source":"data_set_final=pd.DataFrame({\n                         'scene_change_avg':data_set_c[\"scene_change_avg\"] ,\n                         's_duration': data_set_c['s_duration'], \n                         's_video_id':data_set_c['s_video_id'],\n                         'e_framerate':data_set_c['e_framerate'],\n                         's_height':data_set_c['s_height'], \n                         's_storage_size': data_set_c['s_storage_size'], \n                         'Res_WidthHeight': data_set_c['Res_width_height'],\n                         't_average_bitrate': data_set_c['t_average_bitrate'], \n                         'e_crf': data_set_c['e_crf'], \n                         's_width' : data_set_c['s_width'], \n                         't_average_vmaf': data_set_c['t_average_vmaf']})\n\ndata_set_final.dropna(inplace=True)\ndisplay(data_set_final.head())\nprint(\"**Dataset shape after feature selection:**\", data_set_final.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T09:59:09.335646Z","iopub.execute_input":"2022-04-09T09:59:09.336454Z","iopub.status.idle":"2022-04-09T09:59:09.363821Z","shell.execute_reply.started":"2022-04-09T09:59:09.336394Z","shell.execute_reply":"2022-04-09T09:59:09.363220Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Selecting the X as features and y as Target**  ","metadata":{}},{"cell_type":"code","source":"data_set_final1 = data_set_final.drop(['s_video_id'], axis=1) # removing viedo ID for only training \n\nX = np.array(data_set_final1.drop(['t_average_vmaf'], 1)) # selecting the features except VMAF\nX = preprocessing.scale(X) \ny = np.array(data_set_final1['t_average_vmaf']) # Selecting the VMAF as a target \n\nprint(\"The shape of X:\",X.shape)\nprint(\"The shape of y:\",y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T09:59:38.603328Z","iopub.execute_input":"2022-04-09T09:59:38.603918Z","iopub.status.idle":"2022-04-09T09:59:38.615278Z","shell.execute_reply.started":"2022-04-09T09:59:38.603876Z","shell.execute_reply":"2022-04-09T09:59:38.614514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some of the encoded videos of each source video may\nplace in train data and the rest in the test set. In this way,\nit looks unfair because the results probably biased to the\ntraining set, and the trend of each source video would be\npredictable easily so I applied 10-fold cross-validation on dataset since the dataset contains around 6680 data point and also each video content has value around 60, I used K=10 to devided data based on group of viedo content category which is around 600 hundered and the rest would be the other video content category with different feature.     \n\n- **point:** by applying the k-Fold Cross validation on random forest with 1000 trees just try to understand the highest performance of Random forest then I will apply hyper parameter in order to find the optimum of model.    ","metadata":{}},{"cell_type":"code","source":"def Cross_score():\n    \n    SVR_cross = svm.SVR()\n    LR_cross = LinearRegression()\n    RF_cross = RandomForestRegressor(n_estimators=1000)\n\n    scores_SVR = cross_val_score(SVR_cross, X, y, cv=10)\n    scores_LR = cross_val_score(LR_cross, X, y, cv=10) \n    scores_RF = cross_val_score(RF_cross, X, y, cv=10) \n    \n    print('----------------------------------------')\n    print('scores_SVR:',scores_SVR)\n    print('----------------------------------------')\n    print('scores_LR:',scores_LR)\n    print('----------------------------------------')\n    print('scores_RF:',scores_RF)\n    \nCross_score()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T10:00:23.426740Z","iopub.execute_input":"2022-04-09T10:00:23.427045Z","iopub.status.idle":"2022-04-09T10:03:54.801832Z","shell.execute_reply.started":"2022-04-09T10:00:23.427000Z","shell.execute_reply":"2022-04-09T10:03:54.801082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the result of Cross validation, I found that the result of Random forest is showing high performance than the other model. ","metadata":{}},{"cell_type":"markdown","source":"**Checking the normal `train_test_split` on dataset and also check the confidence of each model**","metadata":{}},{"cell_type":"markdown","source":"Approximately 25% of source videos\n(together with all encoded of source videos) are chosen to\nbe the test data, and 75% remaining were using in the\ntraining set. This method ensures that there is not biased to\nthe training set. ","metadata":{}},{"cell_type":"markdown","source":"**Split into Train and Test set**\n","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T10:03:54.803277Z","iopub.execute_input":"2022-04-09T10:03:54.803554Z","iopub.status.idle":"2022-04-09T10:03:54.809044Z","shell.execute_reply.started":"2022-04-09T10:03:54.803523Z","shell.execute_reply":"2022-04-09T10:03:54.808164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Machine learninig model ","metadata":{}},{"cell_type":"code","source":"def ML_model(X_train,y_train,X_test,y_test):\n    \n    SVR = svm.SVR()\n    LR = LinearRegression()\n    RF = RandomForestRegressor(n_estimators=1000)\n\n    SVR.fit(X_train, y_train)\n    LR.fit(X_train, y_train)\n    RF.fit(X_train, y_train)\n    \n    predict_SVR =SVR.predict(X_test)\n    predict_LR=LR.predict(X_test)\n    predict_RF=RF.predict(X_test)\n    \n    Result_svr = pd.concat([pd.DataFrame({\"y_test\":y_test}) ,pd.DataFrame({\"predict_SVR\":predict_SVR})], axis = 1)\n    Result_lr = pd.concat([pd.DataFrame({\"y_test\":y_test}) ,pd.DataFrame({\"predict_LR\":predict_LR})], axis = 1)\n    Result_rf = pd.concat([pd.DataFrame({\"y_test\":y_test}) ,pd.DataFrame({\"predict_RF\":predict_RF})], axis = 1)\n    \n\n    Result_svr.to_csv('SVR.csv')\n    Result_lr.to_csv('LinearRegression.csv')\n    Result_rf.to_csv('RF.csv')\n\n\n\n    confidence_SVR = SVR.score(X_test, y_test)\n    confidence_LR = LR.score(X_test, y_test)\n    confidence_RF = RF.score(X_test, y_test)\n    \n    lst1 =[\"SVR\",\"LR\",\"RF\"]\n    lst = [predict_SVR,predict_LR,predict_RF]\n    for idx, val in enumerate(lst):\n        Corr = np.corrcoef(y_test,val)[0][1]\n        MSE1 = MSE(y_test, val)\n        MAE1 = MAE(y_test, val)\n        RMSE = sqrt(MSE1)\n        R2 = r2_score(y_test,val)\n        \n        print(\"--------------------------\")\n        print('Corr_{}: {:.3f}' .format(lst1[idx],Corr)) \n        print('MAE_{}:  {:.3f}' .format(lst1[idx],MAE1))\n        print('MSE_{}:  {:.3f}' .format(lst1[idx],MSE1))\n        print('RMSE_{}: {:.3f}' .format(lst1[idx],RMSE))\n        print('R2_{}:   {:.3f}' .format(lst1[idx],R2))\n        print(\"--------------------------\")\n        \n        figure = plt.subplots(figsize=(3,3))\n        plt.scatter(y_test,val)\n        plt.xlabel('Actual vmaf')\n        plt.ylabel('Predicted vmaf')\n        plt.title('%s' %lst1[idx])\n\n        \n    print(\"*************Confidence********\")\n    \n    print(\"--------------------------\")\n    print('confidence_SVR:',confidence_SVR)\n    print(\"--------------------------\")\n    print('confidence_LR:',confidence_LR)\n    print(\"--------------------------\")\n    print('confidence_RF:',confidence_RF)\n    print(\"--------------------------\")\n    \n\n\nML_model(X_train,y_train,X_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T10:03:54.810061Z","iopub.execute_input":"2022-04-09T10:03:54.810296Z","iopub.status.idle":"2022-04-09T10:04:15.637665Z","shell.execute_reply.started":"2022-04-09T10:03:54.810270Z","shell.execute_reply":"2022-04-09T10:04:15.636697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since Hyperparameters are directly control the behaviour of the training algorithm and have a significant impact on the performance of the model is being trained, I applied the hyperparameter on best model which is Random Forest to get the optimum parameters of random forest.The train set samples correctly as well as the test set base on the 10-fold cross-validation score; I decided to use a random search to just see how the performance of the random forest model changed.     ","metadata":{}},{"cell_type":"markdown","source":"## Hyper parameter on Random forest Using Randomized Search","metadata":{}},{"cell_type":"code","source":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\nprint(\"*****Random Grid parameter*****\")\ndisplay(random_grid)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-09T10:04:50.633826Z","iopub.execute_input":"2022-04-09T10:04:50.634136Z","iopub.status.idle":"2022-04-09T10:04:50.645998Z","shell.execute_reply.started":"2022-04-09T10:04:50.634100Z","shell.execute_reply":"2022-04-09T10:04:50.645372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Use the random grid to search for best hyperparameters","metadata":{}},{"cell_type":"code","source":"\n# First create the base model to tune\nrf = RandomForestRegressor()\n# Random search of parameters, using 5 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_model = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100,cv = 10, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_model.fit(X_train, y_train)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-09T10:05:14.255808Z","iopub.execute_input":"2022-04-09T10:05:14.256092Z","iopub.status.idle":"2022-04-09T11:19:27.160654Z","shell.execute_reply.started":"2022-04-09T10:05:14.256063Z","shell.execute_reply":"2022-04-09T11:19:27.158762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Here is the best parameter that suit for Random forest model**","metadata":{}},{"cell_type":"code","source":"best=rf_model.best_estimator_\nprint(best)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:19:36.249160Z","iopub.execute_input":"2022-04-09T11:19:36.249387Z","iopub.status.idle":"2022-04-09T11:19:36.254705Z","shell.execute_reply.started":"2022-04-09T11:19:36.249358Z","shell.execute_reply":"2022-04-09T11:19:36.253849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Applying the best estimator for Random Forest model to see the difference** ","metadata":{}},{"cell_type":"code","source":"RF = RandomForestRegressor(n_estimators=400, max_features='sqrt',bootstrap=False)\nRF.fit(X_train, y_train)\npredict_RF=RF.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:20:48.051213Z","iopub.execute_input":"2022-04-09T11:20:48.051567Z","iopub.status.idle":"2022-04-09T11:20:52.769605Z","shell.execute_reply.started":"2022-04-09T11:20:48.051533Z","shell.execute_reply":"2022-04-09T11:20:52.768751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Feature importance from Random Forest model","metadata":{}},{"cell_type":"code","source":"feature_names = list(data_set_final.columns)[:-1]\nimportances = RF.feature_importances_\nfor i,v in enumerate(importances):\n    print('%s,---------------------Score: %.5f' % (feature_names[i],v))","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:20:56.925197Z","iopub.execute_input":"2022-04-09T11:20:56.925483Z","iopub.status.idle":"2022-04-09T11:20:57.028959Z","shell.execute_reply.started":"2022-04-09T11:20:56.925455Z","shell.execute_reply":"2022-04-09T11:20:57.028367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_names = list(data_set_final.columns)[:-1]\nfig = dict({\n    \"data\": [{\"type\": \"bar\",\n              \"x\": feature_names[:-1],\n              \"y\": importances}],\n    \"layout\": {\"title\": {\"text\": \"A Figure Specified Important Features\"},\n               'xaxis': {'categoryorder': 'total ascending'}}\n})\n\nfig = go.Figure(fig)\npy.iplot(fig)","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:21:00.111142Z","iopub.execute_input":"2022-04-09T11:21:00.111701Z","iopub.status.idle":"2022-04-09T11:21:00.147083Z","shell.execute_reply.started":"2022-04-09T11:21:00.111666Z","shell.execute_reply":"2022-04-09T11:21:00.146452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  *** plot 6 esolution curves based on VMAF ***","metadata":{}},{"cell_type":"markdown","source":"By having a closer look at the rate-distortion plots of each\nof the video clips, it can observe that in each specific\nresolution, by increasing the bitrate, the quality enhances\nto a certain point, after passing that point the quality\nbecomes flattened, thus jumping to a higher resolution is\nconvenient.","metadata":{}},{"cell_type":"code","source":"FIGURE, ax = plt.subplots(3, 2,figsize=(13,13))\nuniq = data_set_c['c_content_category'].unique()# get the unique content   \n\n\n\n###################################ax[0, 0]######################################\n# Summerize values which is sorted in uniq\nsummerize = data_set_c[data_set_c['c_content_category'] == uniq[0]]\nresoluion_lst = summerize['e_width'].unique()# Get the unique resolution from our dataset\n\nfor i in resoluion_lst:\n        resoluion = summerize[summerize['e_width'] == i]\n        resoluion = resoluion.sort_values(by=['t_average_bitrate'])\n        x = resoluion['t_average_bitrate']\n        y = resoluion['t_average_vmaf']\n        ax[0, 0].plot(x, y, label=i)\n        ax[0, 0].set_xlabel('Average_Bitrate')\n        ax[0, 0].set_ylabel('Average_VMAF')\n        ax[0, 0].title.set_text(uniq[0])\n\n###################################ax[1, 0]#####################################        \nsummerize=data_set_c[data_set_c['c_content_category'] == uniq[1]]\nresoluion_lst = summerize['e_width'].unique()\nfor i in resoluion_lst:\n        resoluion = summerize[summerize['e_width'] == i]\n        resoluion = resoluion.sort_values(by=['t_average_bitrate'])\n        x = resoluion['t_average_bitrate']\n        y = resoluion['t_average_vmaf']\n        ax[1, 0].plot(x, y, label=i)\n        ax[1, 0].set_xlabel('Average_Bitrate')\n        ax[1, 0].set_ylabel('Average_VMAF')\n        ax[1, 0].title.set_text(uniq[1])\n\n###################################ax[2, 0]###################################\nsummerize=data_set_c[data_set_c['c_content_category'] == uniq[2]]\nresoluion_lst = summerize['e_width'].unique()\nfor i in resoluion_lst:\n        resoluion = summerize[summerize['e_width'] == i]\n        resoluion = resoluion.sort_values(by=['t_average_bitrate'])\n        x = resoluion['t_average_bitrate']\n        y = resoluion['t_average_vmaf']\n        ax[2, 0].plot(x, y, label=i)\n        ax[2, 0].set_xlabel('Average_Bitrate')\n        ax[2, 0].set_ylabel('Average_VMAF')\n        ax[2, 0].title.set_text(uniq[2])\n\n        \n###################################ax[0, 1]###################################\nsummerize=data_set_c[data_set_c['c_content_category'] == uniq[3]]\nresoluion_lst = summerize['e_width'].unique()\nfor i in resoluion_lst:\n        resoluion = summerize[summerize['e_width'] == i]\n        resoluion = resoluion.sort_values(by=['t_average_bitrate'])\n        x = resoluion['t_average_bitrate']\n        y = resoluion['t_average_vmaf']\n        ax[0, 1].plot(x, y, label=i)\n        ax[0, 1].set_xlabel('Average_Bitrate')\n        ax[0, 1].set_ylabel('Average_VMAF')\n        ax[0, 1].title.set_text(uniq[3])\n\n\n###################################ax[1, 1]##################################\nsummerize = data_set_c[data_set_c['c_content_category'] == uniq[4]]\nresoluion_lst = summerize['e_width'].unique()\nfor i in resoluion_lst:\n        resoluion = summerize[summerize['e_width'] == i]\n        resoluion = resoluion.sort_values(by=['t_average_bitrate'])\n        x = resoluion['t_average_bitrate']\n        y = resoluion['t_average_vmaf']\n        ax[1, 1].plot(x, y, label=i)\n        ax[1, 1].set_xlabel('Average_Bitrate')\n        ax[1, 1].set_ylabel('Average_VMAF')\n        ax[1, 1].title.set_text(uniq[4])\n        \n###################################ax[2, 1]##################################\nsummerize = data_set_c[data_set_c['c_content_category'] == uniq[5]]\nresoluion_lst = summerize['e_width'].unique()\nfor i in resoluion_lst:\n        resoluion = summerize[summerize['e_width'] == i]\n        resoluion = resoluion.sort_values(by=['t_average_bitrate'])\n        x = resoluion['t_average_bitrate']\n        y = resoluion['t_average_vmaf']\n        ax[2, 1].plot(x, y, label=i)\n        ax[2, 1].set_xlabel('Average_Bitrate')\n        ax[2, 1].set_ylabel('Average_VMAF')\n        ax[2, 1].title.set_text(uniq[5])\n\n\n\nplt.legend(title='Resolution')\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:21:03.957411Z","iopub.execute_input":"2022-04-09T11:21:03.958348Z","iopub.status.idle":"2022-04-09T11:21:05.072528Z","shell.execute_reply.started":"2022-04-09T11:21:03.958288Z","shell.execute_reply":"2022-04-09T11:21:05.071756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The convex hull is a ubiquitous structure in\ncomputational geometry. Convexity A set S is convex if x\n∈ S and y ∈ S implies that the segment xy ⊆ S.\nmeaning, Given any two points in the polygon, the line\nsegment between them stays inside the polygon, in other\nwords, the convex hull or convex envelope of a set X of\npoints in the Euclidean plane or a Euclidean space is the\nsmallest convex set that contains X","metadata":{}},{"cell_type":"markdown","source":"# ***Convex Hull on  6 resolution of Curves*** ","metadata":{}},{"cell_type":"code","source":"\nfigure, ax = plt.subplots(3, 2,figsize=(13,13))\n\n     \n\n###################################ax[0, 0]###############################\nsummerize=data_set_c[data_set_c['c_content_category']==uniq[0]] \nresoluion_lst = summerize['e_width'].unique() \n\n\nfor i in resoluion_lst:\n        resoluion = summerize[summerize['e_width']==i]\n        resoluion = resoluion.sort_values(by=['t_average_bitrate'])\n        x = resoluion['t_average_bitrate']\n        y = resoluion['t_average_vmaf']\n        ax[0, 0].plot(x, y, label=i)\n        ax[0, 0].set_xlabel('Average_Bitrate')\n        ax[0, 0].set_ylabel('Average_VMAF')\n        ax[0, 0].title.set_text(uniq[0])\n                        \n# finding x and y axis for our convex hull\n\n\nx = summerize['t_average_bitrate']\ny = summerize['t_average_vmaf']\nx_lst = pd.Series(x).values\ny_lst = pd.Series(y).values\nx_lst_S = x_lst.reshape(64,1)\ny_lst_S = y_lst.reshape(64,1)\nxy = np.concatenate((x_lst_S, y_lst_S), axis=1)\n\n# plot convex hull for axis\n\nhull = ConvexHull(xy)\nfor sample in hull.simplices:\n    ax[0, 0].plot(xy[sample, 0], xy[sample, 1], 'r', label=i, linewidth=2)\n\n###################################ax[1, 0]###############################\nsummerize=data_set_c[data_set_c['c_content_category']==uniq[1]]\nresoluion_lst = summerize['e_width'].unique()\n\nfor i in resoluion_lst:\n        resoluion = summerize[summerize['e_width']==i]\n        resoluion = resoluion.sort_values(by=['t_average_bitrate'])\n        x = resoluion['t_average_bitrate']\n        y = resoluion['t_average_vmaf']\n        ax[1, 0].plot(x, y, label=i)\n        ax[1, 0].set_xlabel('Average_Bitrate')\n        ax[1, 0].set_ylabel('Average_VMAF')\n        ax[1, 0].title.set_text(uniq[1])\n                        \n# finding x and y axis for our convex hull\n\nx = summerize['t_average_bitrate']\ny = summerize['t_average_vmaf']\nx_lst = pd.Series(x).values\ny_lst = pd.Series(y).values\nx_lst_S = x_lst.reshape(70,1)\ny_lst_S = y_lst.reshape(70,1)\nxy = np.concatenate((x_lst_S, y_lst_S), axis=1)\n\n\n# plot convex hull for axis\n\nhull = ConvexHull(xy)\nfor sample in hull.simplices:\n    ax[1, 0].plot(xy[sample, 0], xy[sample, 1], 'r', label=i, linewidth=2)\n\n\n###################################ax[2, 0]################################\nsummerize=data_set_c[data_set_c['c_content_category']==uniq[2]]\nresoluion_lst = summerize['e_width'].unique()\n\n\nfor i in resoluion_lst:\n        resoluion = summerize[summerize['e_width']==i]\n        resoluion = resoluion.sort_values(by=['t_average_bitrate'])\n        x = resoluion['t_average_bitrate']\n        y = resoluion['t_average_vmaf']\n        ax[2, 0].plot(x, y, label=i)\n        ax[2, 0].set_xlabel('Average_Bitrate')\n        ax[2, 0].set_ylabel('Average_VMAF')\n        ax[2, 0].title.set_text(uniq[2])\n                        \n# finding x and y axis for convex hull\n\nx = summerize['t_average_bitrate']\ny = summerize['t_average_vmaf']\nx_lst = pd.Series(x).values\ny_lst = pd.Series(y).values\nx_lst_S = x_lst.reshape(415,1)\ny_lst_S = y_lst.reshape(415,1)\nxy = np.concatenate((x_lst_S, y_lst_S), axis=1)\n\n\n## plot convex hull for axis\n\nhull = ConvexHull(xy)\nfor sample in hull.simplices:\n    ax[2, 0].plot(xy[sample, 0], xy[sample, 1], 'r', label=i, linewidth=2)\n\n        \n\n###################################ax[0, 1]###############################\nsummerize=data_set_c[data_set_c['c_content_category']==uniq[3]]\nresoluion_lst = summerize['e_width'].unique()\n\nfor i in resoluion_lst:\n        resoluion = summerize[summerize['e_width']==i]\n        resoluion = resoluion.sort_values(by=['t_average_bitrate'])\n        x = resoluion['t_average_bitrate']\n        y = resoluion['t_average_vmaf']\n        ax[0, 1].plot(x, y, label=i)\n        ax[0, 1].set_xlabel('Average_Bitrate')\n        ax[0, 1].set_ylabel('Average_VMAF')\n        ax[0, 1].title.set_text(uniq[3])\n                        \n# finding x and y axis for our convex hull\n\nx = summerize['t_average_bitrate']\ny = summerize['t_average_vmaf']\nx_lst = pd.Series(x).values\ny_lst = pd.Series(y).values\nx_lst_S = x_lst.reshape(66,1)\ny_lst_S = y_lst.reshape(66,1)\nxy = np.concatenate((x_lst_S, y_lst_S), axis=1)\n\n\n## plot convex hull for axis\n\nhull = ConvexHull(xy)\nfor sample in hull.simplices:\n    ax[0, 1].plot(xy[sample, 0], xy[sample, 1], 'r', label=i, linewidth=2)\n\n\n###################################ax[1, 1]###############################\nsummerize=data_set_c[data_set_c['c_content_category']==uniq[4]] \nresoluion_lst = summerize['e_width'].unique() \n\nfor i in resoluion_lst:\n        resoluion = summerize[summerize['e_width']==i]\n        resoluion = resoluion.sort_values(by=['t_average_bitrate'])\n        x = resoluion['t_average_bitrate']\n        y = resoluion['t_average_vmaf']\n        ax[1, 1].plot(x, y, label=i)\n        ax[1, 1].set_xlabel('Average_Bitrate')\n        ax[1, 1].set_ylabel('Average_VMAF')\n        ax[1, 1].title.set_text(uniq[4])\n                        \n# finding x and y axis for our convex hull\n\nx = summerize['t_average_bitrate']\ny = summerize['t_average_vmaf']\nx_lst = pd.Series(x).values\ny_lst = pd.Series(y).values\nx_lst_S = x_lst.reshape(350,1)\ny_lst_S = y_lst.reshape(350,1)\nxy = np.concatenate((x_lst_S, y_lst_S), axis=1)\n\n\n## plot convex hull for axis\n\nhull = ConvexHull(xy)\nfor sample in hull.simplices:\n    ax[1, 1].plot(xy[sample, 0], xy[sample, 1], 'r', label=i, linewidth=2)\n\n    \n\n###################################ax[2, 1]###############################\nsummerize=data_set_c[data_set_c['c_content_category']==uniq[5]]\nresoluion_lst = summerize['e_width'].unique()\n\nfor i in resoluion_lst:\n        resoluion = summerize[summerize['e_width']==i]\n        resoluion = resoluion.sort_values(by=['t_average_bitrate'])\n        x = resoluion['t_average_bitrate']\n        y = resoluion['t_average_vmaf']\n        ax[2, 1].plot(x, y, label=i)\n        ax[2, 1].set_xlabel('Average_Bitrate')\n        ax[2, 1].set_ylabel('Average_VMAF')\n        ax[2, 1].title.set_text(uniq[5])\n                        \n# finding x and y axis for our convex hull\n\nx = summerize['t_average_bitrate']\ny = summerize['t_average_vmaf']\nx_lst = pd.Series(x).values\ny_lst = pd.Series(y).values\nx_lst_S = x_lst.reshape(139,1)\ny_lst_S = y_lst.reshape(139,1)\nxy = np.concatenate((x_lst_S, y_lst_S), axis=1)\n\nplt.legend(title='Resolution') \n## plot convex hull for axis\nhull = ConvexHull(xy)\nfor sample in hull.simplices:\n    ax[2, 1].plot(xy[sample, 0], xy[sample, 1], 'r', label=i, linewidth=2)\n\n\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-04-09T11:21:12.768621Z","iopub.execute_input":"2022-04-09T11:21:12.768937Z","iopub.status.idle":"2022-04-09T11:21:14.079274Z","shell.execute_reply.started":"2022-04-09T11:21:12.768905Z","shell.execute_reply":"2022-04-09T11:21:14.078252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* # Conclusion","metadata":{}},{"cell_type":"markdown","source":"**I built a parametric based model that can\npredict the convex hull using encoding parameters as well\nas content information such as scene change information.\nIn addition, I compared different types of regression\nmodels to find the best model that predict VMAF. In this\nway, I first processed the raw data by removing outliers,\nhandle the missing values and duplicates values.**","metadata":{}},{"cell_type":"markdown","source":"**For Choosing the right VMAF based on convexhull is applying on real test-bed.** ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}